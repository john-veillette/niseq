{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a91a95",
   "metadata": {},
   "source": [
    "# Verifying false-positive rates by simulation\n",
    "\n",
    "This notebook contains simulations to show that the permutation-based approach to group-sequential testing used in this package effectively controls the false-positive rate, even when analyzing the data sequentially (i.e. stopping data collection when the pattern of interest is significant, and continuing otherwise). \n",
    "\n",
    "In these simulations, we apply the permutation scheme to univariate data for ease of computation -- but the nice thing about permutation tests is that they can be used on arbitrary test statistics, so the results here can generalize to e.g. a cluster statistic. To perform permutations and compute adjusted thresholds, we'll use functions from the `niseq.max_test` module, which perform permutation _t_-tests. However, the same backend functions, `niseq._permutation.generate_permutation_dist` and `niseq._permutation.find_thresholds`, are used under the hood of every statistical test implemented in `niseq`, so you can be sure the same permutation scheme is being used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27fa087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.parallel import parallel_func\n",
    "from niseq.max_test import (\n",
    "    sequential_permutation_t_test_1samp, \n",
    "    sequential_permutation_test_indep\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "N_SIMULATIONS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832032d",
   "metadata": {},
   "source": [
    "On each simulation below, we'll generate null data and pretend we look at it five times throughout the intended course of data collection, and we'll compare the false positive rates attained when we reject the null hypothesis whenever $p \\leq \\alpha$ and $p \\leq \\alpha_\\text{adjusted}$ at at least one look time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1374ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_simulation(seed, tail = 0, indep = False,\n",
    "                   look_times = np.linspace(100, 500, 5).astype(int)):\n",
    "    \n",
    "    # generate null data \n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = rng.normal(loc = 0, size = look_times[-1])\n",
    "    \n",
    "    # run sequential test\n",
    "    if indep:\n",
    "        conds = rng.choice([0, 1], look_times[-1])\n",
    "        _, p, adj_alpha, _ = sequential_permutation_test_indep(\n",
    "            x, conds, look_times, n_max = look_times[-1], \n",
    "            tail = tail,\n",
    "            seed = seed\n",
    "        ) \n",
    "    else:\n",
    "        _, p, adj_alpha, _ = sequential_permutation_t_test_1samp(\n",
    "            x, look_times, n_max = look_times[-1], \n",
    "            tail = tail,\n",
    "            seed = seed\n",
    "        ) \n",
    "        \n",
    "    # reject if p-val crosses sig threshold at any look time\n",
    "    return np.array([np.any(p < .05), np.any(p < adj_alpha)]) \n",
    "\n",
    "\n",
    "def run_simulations(n_simulations, tail = 0, indep = False, n_jobs = -1):\n",
    "    parallel, p_func, _ = parallel_func(one_simulation, n_jobs)\n",
    "    out = parallel(p_func(seed, tail, indep) for seed in range(n_simulations))\n",
    "    rejections = np.stack(out)\n",
    "    fpr = rejections.mean(0)\n",
    "    print('False positive rate without correction: ' + str(fpr[0]))\n",
    "    print('False positive rate *with* correction: ' + str(fpr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe58ea",
   "metadata": {},
   "source": [
    "## One Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a93d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   45.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate without correction: 0.145\n",
      "False positive rate *with* correction: 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.7min finished\n"
     ]
    }
   ],
   "source": [
    "run_simulations(N_SIMULATIONS, indep = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ac8e0",
   "metadata": {},
   "source": [
    "If we don't correct for sequential looks using our alpha spending approach, rejecting the null and stopping data collection as soon as we see $p \\leq 0.05$ but continuing to collect data otherwise, we end up with an inflated false positive rate. But using our adjusted thresholds, false positive rates are contains below our target $\\alpha = 0.05$, even if we stop data collection on the first look where the data pass the threshold! Thus, as long as we can specify a reasonable $n_\\text{max}$ we're willing to collect, we can use this procedure to determine our sample size adaptively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d34bbc",
   "metadata": {},
   "source": [
    "## Independent Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0633e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  3.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate without correction: 0.148\n",
      "False positive rate *with* correction: 0.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  4.5min finished\n"
     ]
    }
   ],
   "source": [
    "# simulate false-positive rate for two-sample test\n",
    "run_simulations(N_SIMULATIONS, indep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5daa64",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "[Similar simulations](https://github.com/john-veillette/niseq/blob/main/niseq/tests/test_permutation.py) are run whenever new code is added to `niseq` as part our continuous integration pipeline to ensure the permutation scheme continues to control the false positive rate following any changes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
