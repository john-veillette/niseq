{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a91a95",
   "metadata": {},
   "source": [
    "# Verifying false-positive rates by simulation\n",
    "\n",
    "This notebook contains simulations to show that the permutation-based approach to group-sequential testing used in this package effectively controls the false-positive rate, even when analyzing the data sequentially (i.e. stopping data collection when the pattern of interest is significant, and continuing otherwise). \n",
    "\n",
    "In these simulations, we'll use functions from the `niseq.max_test` module, which perform permutation _t_-tests with a $t$-max correction for multiple comparisons, to illustrate how `niseq` can control the familywise error rate across multiple looks during data collection. While we don't show simulations for our cluster-based tests here (those take a lot longer to run), note that the same backend functions, `niseq._permutation.generate_permutation_dist` and `niseq._permutation.find_thresholds`, are used under the hood of every statistical test implemented in `niseq`, so the permutation scheme scrutinized here is the exact same as that used in our cluster-level tests.\n",
    "\n",
    "Note that these simulations will only give us approximate results. Feel free to increase `N_SIMULATIONS` to get something closer to the true false-positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27fa087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.parallel import parallel_func\n",
    "from niseq.max_test import (\n",
    "    sequential_permutation_t_test_1samp, \n",
    "    sequential_permutation_test_indep\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "N_SIMULATIONS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832032d",
   "metadata": {},
   "source": [
    "On each simulation below, we'll generate null data and pretend we look at it five times throughout the intended course of data collection, and we'll compare the false positive rates attained when we reject the null hypothesis whenever $p \\leq 0.05$ and $p \\leq \\alpha_\\text{adjusted}$ at at least one look time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1374ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_simulation(seed, \n",
    "                tail = 0, \n",
    "                indep = False,\n",
    "                look_times = np.linspace(100, 500, 5).astype(int),\n",
    "                n_tests = 100):\n",
    "    \n",
    "    # generate null data \n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = rng.normal(loc = 0, size = (look_times[-1], n_tests))\n",
    "    \n",
    "    # run sequential test\n",
    "    if indep:\n",
    "        conds = rng.choice([0, 1], look_times[-1])\n",
    "        _, p, adj_alpha, _ = sequential_permutation_test_indep(\n",
    "            x, conds, look_times, n_max = look_times[-1], \n",
    "            tail = tail,\n",
    "            verbose = 0,\n",
    "            seed = seed\n",
    "        ) \n",
    "    else:\n",
    "        _, p, adj_alpha, _ = sequential_permutation_t_test_1samp(\n",
    "            x, look_times, n_max = look_times[-1], \n",
    "            tail = tail,\n",
    "            verbose = 0,\n",
    "            seed = seed\n",
    "        ) \n",
    "        \n",
    "    # reject if p-val crosses sig threshold at any look time\n",
    "    return np.array([np.any(p < .05), np.any(p < adj_alpha)]) \n",
    "\n",
    "\n",
    "def run_simulations(n_simulations, tail = 0, indep = False, n_jobs = -1):\n",
    "    parallel, p_func, _ = parallel_func(one_simulation, n_jobs)\n",
    "    out = parallel(p_func(seed, tail, indep) for seed in range(n_simulations))\n",
    "    rejections = np.stack(out)\n",
    "    fpr = rejections.mean(0)\n",
    "    print('False positive rate without correction: ' + str(fpr[0]))\n",
    "    print('False positive rate *with* correction: ' + str(fpr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe58ea",
   "metadata": {},
   "source": [
    "## One Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a93d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  5.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate without correction: 0.163\n",
      "False positive rate *with* correction: 0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.5min finished\n"
     ]
    }
   ],
   "source": [
    "run_simulations(N_SIMULATIONS, indep = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ac8e0",
   "metadata": {},
   "source": [
    "If we don't correct for sequential looks using our alpha spending approach, rejecting the null and stopping data collection as soon as we see $p \\leq 0.05$ but continuing to collect data otherwise, we end up with an inflated false positive rate. But using our adjusted thresholds, familywise error rates are contained below our target $\\alpha = 0.05$, even if we stop data collection on the first look where the test statistic passes the significance threshold! Thus, as long as we can specify a reasonable $n_\\text{max}$ we're willing to collect, we can use this procedure to determine our sample size adaptively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d34bbc",
   "metadata": {},
   "source": [
    "## Independent Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0633e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   39.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  8.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate without correction: 0.175\n",
      "False positive rate *with* correction: 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  9.4min finished\n"
     ]
    }
   ],
   "source": [
    "# simulate false-positive rate for two-sample test\n",
    "run_simulations(N_SIMULATIONS, indep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5daa64",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "[Similar simulations](https://github.com/john-veillette/niseq/blob/main/niseq/tests/test_permutation.py) are run whenever new code is added to `niseq` as part our continuous integration pipeline to ensure the permutation scheme continues to control the false positive rate following any changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd23a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
